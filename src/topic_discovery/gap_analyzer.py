"""STORM multi-perspective questioning for research gap discovery.

Implements the STORM pattern: multiple perspective agents interrogate a corpus
of papers from different disciplinary angles, generating questions that expose
under-explored areas and cross-language blind spots.
"""

from __future__ import annotations

import json
import logging
from typing import Any

from src.knowledge_base.models import Paper
from src.llm.router import LLMRouter

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Perspective agent definitions
# ---------------------------------------------------------------------------

PERSPECTIVE_AGENTS: list[dict[str, str]] = [
    {
        "name": "Comparatist",
        "system_prompt": (
            "You are a specialist in world literature and translation studies. "
            "You approach every corpus by asking: What gets lost or distorted when "
            "literary works cross linguistic and cultural boundaries? How do "
            "translation practices, publishing markets, and academic institutions "
            "shape which works circulate and how they are read? You are especially "
            "attentive to power asymmetries between literary traditions and to "
            "methodological assumptions that privilege one national canon over others."
        ),
    },
    {
        "name": "Chinese Literature Specialist",
        "system_prompt": (
            "You are a specialist in Chinese literature from classical to "
            "contemporary periods. You read primary texts in Chinese and are deeply "
            "familiar with Chinese-language scholarship (published in journals such "
            "as Wenxue Pinglun, Zhongguo Bijiao Wenxue, and Wenyi Yanjiu). You "
            "notice when Western-language scholarship overlooks debates that are "
            "well-established in Chinese academia, and you flag topics where Chinese "
            "critical frameworks diverge from Euro-American ones."
        ),
    },
    {
        "name": "French Literature Specialist",
        "system_prompt": (
            "You are a specialist in French and Francophone literature. You read "
            "primary texts in French and follow French-language scholarship in "
            "journals such as Revue de Litterature Comparee, Poetique, and "
            "Litteratures. You are alert to the distinctive traditions of French "
            "literary theory (narratology, genetique des textes, sociocritique) and "
            "notice when Anglophone scholarship reinvents concepts already developed "
            "in French-language criticism, or ignores Francophone perspectives."
        ),
    },
    {
        "name": "Methodology Critic",
        "system_prompt": (
            "You are a methodological critic who evaluates the rigor and innovation "
            "of research designs in literary studies. You scrutinize how scholars "
            "build arguments, select evidence, and position themselves relative to "
            "existing theory. You ask whether digital humanities methods, "
            "quantitative approaches, or interdisciplinary frameworks could open up "
            "new lines of inquiry. You are skeptical of purely impressionistic "
            "readings and push for reproducible, well-grounded analysis."
        ),
    },
]

# ---------------------------------------------------------------------------
# Prompt templates
# ---------------------------------------------------------------------------

_QUESTION_PROMPT = """\
You are reviewing the following corpus of {count} scholarly papers in comparative \
literature. Each paper is listed with its title, authors, journal, year, language, \
and keywords.

--- CORPUS ---
{corpus_summary}
--- END CORPUS ---

From your perspective as a {perspective_name}, generate 5 to 8 probing questions \
that identify potential research gaps, under-explored connections, or methodological \
blind spots in this corpus. Focus on what is MISSING or insufficiently addressed.

Return your questions as a JSON array of strings. Output ONLY the JSON array, no \
other text.
"""

_SYNTHESIS_PROMPT = """\
You are a senior comparatist synthesizing research gap analyses from multiple \
disciplinary perspectives. Below are sets of probing questions generated by four \
different perspective agents examining the same corpus of papers.

{perspective_questions}

Synthesize these questions into a structured list of concrete research gaps. For \
each gap, provide:
- "title": a concise label (max 15 words)
- "description": a 2-3 sentence explanation of the gap
- "evidence": what in the corpus (or absent from it) suggests this gap exists
- "perspectives": which perspective agents flagged related questions
- "potential_rq": a draft research question that could address this gap

Return your answer as a JSON array of objects with exactly these five keys. \
Output ONLY the JSON array, no other text.
"""


# ---------------------------------------------------------------------------
# Helper utilities
# ---------------------------------------------------------------------------


def _build_corpus_summary(papers: list[Paper], max_papers: int = 80) -> str:
    """Create a compact textual summary of the paper corpus for LLM context."""
    lines: list[str] = []
    for p in papers[:max_papers]:
        kw = ", ".join(p.keywords) if p.keywords else "none"
        lines.append(
            f"- [{p.language.value.upper()}] {p.title} | "
            f"{', '.join(p.authors[:3])} | {p.journal} ({p.year}) | kw: {kw}"
        )
    if len(papers) > max_papers:
        lines.append(f"... and {len(papers) - max_papers} more papers")
    return "\n".join(lines)


def _parse_json_array(text: str) -> list[Any]:
    """Robustly extract a JSON array from LLM output."""
    text = text.strip()
    # Find the first '[' and last ']' to handle preamble/postamble
    start = text.find("[")
    end = text.rfind("]")
    if start == -1 or end == -1:
        logger.warning("No JSON array found in LLM response, returning empty list")
        return []
    try:
        return json.loads(text[start : end + 1])
    except json.JSONDecodeError as exc:
        logger.warning("Failed to parse JSON from LLM response: %s", exc)
        return []


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


async def analyze_gaps(
    papers: list[Paper],
    llm_router: LLMRouter,
) -> list[dict]:
    """Run STORM multi-perspective questioning to discover research gaps.

    Each of the four perspective agents generates probing questions about the
    corpus.  A synthesis step then merges the questions into structured gap
    descriptions.

    Parameters
    ----------
    papers:
        The corpus of papers to analyze.
    llm_router:
        LLM router instance (uses task_type="topic_discovery").

    Returns
    -------
    list[dict]
        Each dict has keys: title, description, evidence, perspectives,
        potential_rq.
    """
    if not papers:
        logger.warning("analyze_gaps called with empty paper list")
        return []

    corpus_summary = _build_corpus_summary(papers)

    # --- Phase 1: Each perspective agent generates questions ----------------
    perspective_results: dict[str, list[str]] = {}

    for agent in PERSPECTIVE_AGENTS:
        user_prompt = _QUESTION_PROMPT.format(
            count=len(papers),
            corpus_summary=corpus_summary,
            perspective_name=agent["name"],
        )
        messages = [
            {"role": "system", "content": agent["system_prompt"]},
            {"role": "user", "content": user_prompt},
        ]

        try:
            response = llm_router.complete(
                task_type="topic_discovery",
                messages=messages,
                temperature=0.7,
            )
            raw_text = llm_router.get_response_text(response)
            questions = _parse_json_array(raw_text)
            if questions:
                perspective_results[agent["name"]] = [
                    str(q) for q in questions
                ]
                logger.info(
                    "Perspective '%s' generated %d questions",
                    agent["name"],
                    len(questions),
                )
            else:
                logger.warning(
                    "Perspective '%s' returned no parseable questions",
                    agent["name"],
                )
        except Exception:
            logger.exception(
                "LLM call failed for perspective '%s'", agent["name"]
            )

    if not perspective_results:
        logger.error("All perspective agents failed; no gaps to synthesize")
        return []

    # --- Phase 2: Synthesize into structured gaps --------------------------
    formatted_perspectives: list[str] = []
    for name, questions in perspective_results.items():
        q_list = "\n".join(f"  {i + 1}. {q}" for i, q in enumerate(questions))
        formatted_perspectives.append(f"### {name}\n{q_list}")

    synthesis_prompt = _SYNTHESIS_PROMPT.format(
        perspective_questions="\n\n".join(formatted_perspectives)
    )
    messages = [
        {
            "role": "system",
            "content": (
                "You are a senior researcher in comparative literature. "
                "Return well-structured JSON only."
            ),
        },
        {"role": "user", "content": synthesis_prompt},
    ]

    try:
        response = llm_router.complete(
            task_type="topic_discovery",
            messages=messages,
            temperature=0.3,
        )
        raw_text = llm_router.get_response_text(response)
        gaps = _parse_json_array(raw_text)
    except Exception:
        logger.exception("Synthesis LLM call failed")
        gaps = []

    # Validate structure of each gap dict
    required_keys = {"title", "description", "evidence", "perspectives", "potential_rq"}
    validated: list[dict] = []
    for gap in gaps:
        if isinstance(gap, dict) and required_keys.issubset(gap.keys()):
            validated.append(gap)
        else:
            logger.warning("Dropping malformed gap entry: %s", gap)

    logger.info("Gap analysis complete: %d gaps identified", len(validated))
    return validated
